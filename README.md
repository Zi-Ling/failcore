[ä¸­æ–‡](README_ZH.md) | English
# FailCore

> **The "Safety Airbag" for AI Agents.** ðŸ›¡ï¸  
> **Status:** Beta (0.1.x) Â· **Pip Install:** `failcore` Â· **License:** Apache 2.0

[![PyPI version](https://badge.fury.io/py/failcore.svg)](https://badge.fury.io/py/failcore)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

**When your agent breaks, you don't need better prompts â€” you need a circuit breaker.**

FailCore is a **fail-fast execution runtime** for AI agents.  
It does not try to make agents smarter â€” it makes them **safe and reliable**.

While frameworks like LangChain focus on *planning*, FailCore focuses on what happens **during execution**:
enforcing permissions, blocking side-effects (network & filesystem), and generating forensic audit logs.

---

## ðŸ“¸ See It In Action (Forensic Report)

FailCore automatically generates forensic HTML reports for every run.  
*(Below: FailCore blocking a real-world path traversal attack generated by an LLM)*

![FailCore Forensic Report](/docs/images/report_screenshot.png)

> *The red "BLOCKED" status confirms that the agent attempted an unauthorized operation, which FailCore intercepted at the validation layer.*

---

## âœ¨ What's New (v0.1.x)

- ðŸ›¡ï¸ **SSRF Protection** â€” Network-layer validation (DNS resolution and private IP checks).
- ðŸ“‚ **Filesystem Sandbox** â€” Detects and blocks `../` path traversal attacks.
- ðŸ“Š **Audit Reports** â€” One-command generation of professional HTML dashboards.
- ðŸŽ¯ **Semantic Status** â€” Clear distinction between `BLOCKED` (threat neutralized) vs `FAIL` (tool error).

---

## ðŸ”¥ Quick Start

### 1. Install

```bash
pip install failcore
```

### 2. Protect Your Tools (Zero-Touch)

Wrap your existing functions (or LangChain tools) with a FailCore Session.

```python
from failcore import Session, presets

# Enable strict security mode (network & sandbox protection ON)
session = Session(
    validator=presets.fs_safe(strict=True),
    sandbox="./workspace"  # sandbox
)

@session.tool  # 
def write_file(path: str, content: str):
    with open(path, "w") as f:
        f.write(content)

# --- Simulation: LLM tries to attack ---
result = session.call("write_file", path="../etc/passwd", content="hack")

print(f"Status: {result.status}")       # BLOCKED
print(f"Error: {result.error.message}") # Path traversal detected
```

### 3. Generate Audit Report

```bash
failcore show
failcore report --last > report.html
```

---

## Why FailCore?

Modern AI agents are fragile. FailCore addresses core execution risks:

| Risk | Without FailCore | With FailCore |
|------|------------------|---------------|
| **Security (SSRF)** | Agent can access internal metadata services. | **BLOCKED** by network-layer validation. |
| **Filesystem** | Agent can read/write arbitrary files via `../`. | **BLOCKED** by strict sandbox enforcement. |
| **Cost** | One step fails, entire workflow restarts. | **DETERMINISTIC REPLAY** of successful steps. |
| **Visibility** | Thousands of log lines. | **FORENSIC REPORT** with clear verdicts. |

---

## LangChain Integration

```python
from failcore import Session, presets
from failcore.adapters.langchain import map_langchain_tool

session = Session(validator=presets.fs_safe(strict=True))
safe_tool_spec = map_langchain_tool(my_langchain_tool)
session.invoker.register_spec(safe_tool_spec)
```

---

## Contributing

Contributions are welcome.  
If you are building agent systems that need stronger execution guarantees, we would love your feedback.

---

## License

Apache License 2.0 â€” see [LICENSE](LICENSE).

Copyright Â© 2025 ZiLing
